{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import identification_py2 as ident_tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import control as con\n",
    "import glob #for returning files having the specified path extension\n",
    "import statistics as stats\n",
    "import os #checking for empty file\n",
    "%pylab inline\n",
    "\n",
    "from sysident import loadtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Passing all the data into arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_first        = sorted(glob.glob('step_log_new/*/*task*.log')) #corresponds to .log files that has data related to the first position\n",
    "control_first     = sorted(glob.glob('step_log_new/*/*control*.log'))\n",
    "task_remaining    = sorted(glob.glob('step_log_new/*/*task*.log.*')) #corresponds to remaining log.'n' files\n",
    "control_remaining = sorted(glob.glob('step_log_new/*/*control*.log.*'))\n",
    "task              = sorted(task_first + task_remaining) #set of all task_velocity logs\n",
    "control           = sorted(control_first + control_remaining) #set of all control logs\n",
    "observations      = len(task_first) #total number of experiments conducted/observations taken\n",
    "positions         = int(len(task) / observations) #number of points in the given task space\n",
    "task_full         = [] #A task_velocity list whose each element is a list of similar log files i.e from the same position\n",
    "control_full      = [] #A control_output list whose each element is a list of similar log files i.e from the same position\n",
    "\n",
    "for i in range(0, positions):\n",
    "    task_full.append([])\n",
    "    control_full.append([])\n",
    "    for j in range(0, observations):\n",
    "        task_full[i].append(task[i + (j * positions)])\n",
    "        control_full[i].append(control[i + (j * positions)])\n",
    "\n",
    "count = 0 #counter that returns the number of empty files\n",
    "for i in range(0, positions):\n",
    "    for j in range(0, observations):\n",
    "        if os.stat(task_full[i][j]).st_size == 0:\n",
    "            count = count + 1\n",
    "            \n",
    "for i in range(0, positions):\n",
    "    for j in range(0, observations-count):\n",
    "        if os.stat(task_full[i][j]).st_size == 0:\n",
    "            del(task_full[i][j])\n",
    "            del(control_full[i][j])\n",
    "            \n",
    "# Reading all the data into a dataframe array\n",
    "df_ist_soll = []\n",
    "model_estimations = []\n",
    "\n",
    "for i in range(0, positions):\n",
    "    df_ist_soll.append([])\n",
    "    model_estimations.append([])\n",
    "    for j in range(0, observations):\n",
    "        try:\n",
    "            # initialize result dictionary\n",
    "            model_estimations[i].append({})\n",
    "            df_ist_soll[i].append(ident_tools.batch_read_data(control_full[i][j], task_full[i][j]))\n",
    "        except:\n",
    "            print \"Some Error occured and was ignored\"\n",
    "            continue\n",
    "            \n",
    "print \"############################### INFO #########################################\"\n",
    "print \"df_ist_soll contains all reference and actual data per each position and each experiement\"\n",
    "print\n",
    "print \"df_ist_soll[position][experiement] -> pandas.Dataframe{'x_ist', 'x_soll'}\"\n",
    "print \"model_estimations[position][experiement] -> dict{'delay', 'scale_factor', ...}\"\n",
    "print \"############################### INFO #########################################\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cut away leading zeros -> starts directly with step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_pos in enumerate(df_ist_soll):\n",
    "    for j, df_exp in enumerate(df_pos):\n",
    "\n",
    "        df_shifted = df_exp.loc[df_exp.x_soll != 0.0]\n",
    "        df_shifted.index = df_shifted.index - df_shifted.index[0]\n",
    "\n",
    "        df_ist_soll[i][j] = df_shifted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Resample to equal time distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_pos in enumerate(df_ist_soll):\n",
    "    for j, df_exp in enumerate(df_pos):\n",
    "\n",
    "        #print df_exp.head()\n",
    "        #print df_exp.tail()\n",
    "        #ax0 = df_exp.plot()\n",
    "\n",
    "        # Convert to pandas timeseries \n",
    "        df_ist_soll[i][j].set_index(pd.to_datetime(df_ist_soll[i][j].index, unit='s'), inplace=True)\n",
    "        #ax = df_exp.plot()\n",
    "        #print df_exp.head()\n",
    "\n",
    "        # Resample pad and fill NaNs\n",
    "        df_ist_soll[i][j] = df_ist_soll[i][j].resample('1ms').pad().bfill()\n",
    "        #df_exp.plot(ax=ax)\n",
    "\n",
    "        #print df_exp.head()\n",
    "        #print df_exp.tail()\n",
    "\n",
    "        # Revert so numerical format index\n",
    "        #df_exp['datetime'] = df_exp.index\n",
    "        df_ist_soll[i][j]['time'] = pd.to_numeric(df_ist_soll[i][j].index, downcast='float')/1e9\n",
    "        df_ist_soll[i][j].set_index('time', inplace=True)\n",
    "\n",
    "        #df_exp.plot(ax=ax0)\n",
    "        #print df_ist_soll[0][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Displaying all the observations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The first try except code avoids the errors arising due to the already existing Overview directory. \n",
    "# The second try except code avoids the errors resulting from the plotting of the empty data file \n",
    "try:\n",
    "    os.makedirs('View_Data/')\n",
    "except OSError, e:\n",
    "    if e.errno != os.errno.EEXIST:\n",
    "        raise   \n",
    "    pass\n",
    "\n",
    "for i in range(0, positions):\n",
    "    fig = plt.figure(figsize = (10,30))\n",
    "    fig.suptitle('Position %s'%(i + 1), fontsize = 20, fontweight = 'bold')\n",
    "    for j in range(0, observations): \n",
    "        try:\n",
    "            ax = fig.add_subplot(observations, 1, j + 1)\n",
    "            ax.set_title('Observation %s'%(j + 1))\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.plot(df_ist_soll[i][j]) \n",
    "        except:\n",
    "            pass\n",
    "    #plt.savefig('View_Data/Position %s.png'%(i + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Smoothing using Savgol filter and normalization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_pos in enumerate(df_ist_soll):\n",
    "    for j, df_exp in enumerate(df_pos):\n",
    "        \n",
    "        # The following expects the unit step to be single and alone per experiement!\n",
    "        if len(df_exp.x_soll.value_counts()) > 2:\n",
    "            print \"WARNING! - Data in Position {0}, Experiement {1} (df_ist_soll[{0}][{1}])\".format(i, j)\n",
    "            print \"has more than {} different values\". format(len(df_exp.x_soll.value_counts()))\n",
    "            print \"!!!! This could lead to wrong calculations!!!!\"\n",
    "            print \"!!!! Target values should be either Zero (beginning) or step height (until end)!!!!\"\n",
    "            print df_exp.x_soll.value_counts()\n",
    "\n",
    "        factor = 1.0/max(df_exp.x_soll)\n",
    "        model_estimations[i][j]['step_height'] = max(df_exp.x_soll)\n",
    "        model_estimations[i][j]['scale_factor'] = factor\n",
    "        df_ist_soll[i][j] = pd.concat(\n",
    "            [df_ist_soll[i][j],\n",
    "            pd.DataFrame(\n",
    "                factor*np.array(df_exp.x_ist),\n",
    "                columns=['normalized'],\n",
    "                index=df_exp.index)],\n",
    "            axis=1)\n",
    "        \n",
    "        for order in range(1, 4):\n",
    "            df_ist_soll[i][j] = pd.concat(\n",
    "                [df_ist_soll[i][j],\n",
    "                pd.DataFrame(\n",
    "                    ident_tools.smooth(factor*np.array(df_exp.x_ist), order),\n",
    "                    columns=['smooth_{}'.format(order)],\n",
    "                    index=df_exp.index)],\n",
    "                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ist_soll[0][0].plot()\n",
    "print df_ist_soll[0][0].head()\n",
    "\n",
    "print model_estimations[0][0]\n",
    "print df_ist_soll[0][0].keys()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PT1 + PT2 Estimation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, df_pos in enumerate(df_ist_soll):\n",
    "    for j, df_exp in enumerate(df_pos):\n",
    "        \n",
    "        for order in range(1,4):\n",
    "            # PT1 Estimations\n",
    "            tf, _, _, delay, time_constant, steady_state, tf_without_delay =\\\n",
    "                ident_tools.pt1(np.array(df_exp['smooth_{}'.format(order)]), np.array(df_exp.index))\n",
    "            pt1_dict = {\n",
    "                'tf_delay': tf,\n",
    "                'tf': tf_without_delay,\n",
    "                'delay': delay,\n",
    "                'time_constant': time_constant,\n",
    "                'steady_state': steady_state\n",
    "            }\n",
    "            model_estimations[i][j]['pt1_smooth_{}'.format(order)] = pt1_dict\n",
    "        \n",
    "        for order in range(1,4):\n",
    "            try:\n",
    "                tf, _, _, delay, time_constant, steady_state, zeta, tf_without_delay =\\\n",
    "                    ident_tools.pt2(np.array(df_exp['smooth_{}'.format(order)]), np.array(df_exp.index))\n",
    "\n",
    "                pt2_dict = {\n",
    "                    'tf_delay': tf,\n",
    "                    'tf': tf_without_delay,\n",
    "                    'delay': delay,\n",
    "                    'time_constant': time_constant,\n",
    "                    'steady_state': steady_state,\n",
    "                    'zeta': zeta,\n",
    "                }\n",
    "                model_estimations[i][j]['pt2_smooth_{}'.format(order)] = pt2_dict\n",
    "            except:\n",
    "                print \"PT2 Estimation failed (smoothed_{2})! Position {0}, Experiement {1}; df_ist_soll[{0}][{1}]\".format(i, j, order)\n",
    "                #model_estimations[i][j]['pt2_smooth_{}'.format(order)] = None\n",
    "\n",
    "        try:\n",
    "            tf, _, _, delay, time_constant, steady_state, zeta, tf_without_delay =\\\n",
    "                ident_tools.pt2(np.array(df_exp['normalized']), np.array(df_exp.index))\n",
    "\n",
    "            pt2_dict = {\n",
    "                'tf_delay': tf,\n",
    "                'tf': tf_without_delay,\n",
    "                'delay': delay,\n",
    "                'time_constant': time_constant,\n",
    "                'steady_state': steady_state,\n",
    "                'zeta': zeta,\n",
    "            }\n",
    "            model_estimations[i][j]['pt2'] = pt2_dict\n",
    "        except:\n",
    "            print \"PT2 Estimation failed (normalized)! Position {0}, Experiement {1}; df_ist_soll[{0}][{1}]\".format(i, j)\n",
    "            #model_estimations[i][j]['pt2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print model_estimations[0][0].keys()\n",
    "print model_estimations[0][0]['pt2_smooth_2'].keys()\n",
    "\n",
    "t0, y0 = con.step_response(model_estimations[0][0]['pt2_smooth_2']['tf'])\n",
    "t1, y1 = con.step_response(model_estimations[0][0]['pt1_smooth_1']['tf_delay'])\n",
    "t2, y2 = con.step_response(model_estimations[0][0]['pt2_smooth_2']['tf_delay'])\n",
    "\n",
    "### Plotting\n",
    "plot(t0+model_estimations[0][0]['pt2_smooth_2']['delay'], y0, label='shifted pt2')\n",
    "#plot(t0, y0, label='unshifted pt2')\n",
    "plot(t1, y1, label='pt1d')\n",
    "plot(t2, y2, label='pt2d')\n",
    "plot(df_ist_soll[0][0]['normalized'], label='data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Average values over position and experiement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def get_mse(y, t, df_ist_soll):\n",
    "    try:\n",
    "        df_sysout = pd.DataFrame(y, index=pd.to_datetime(t, unit='s'))\n",
    "        df_sysout = df_sysout.drop_duplicates(df_sysout.index)\n",
    "        df_sysout = df_sysout.resample('1ms').pad().bfill()\n",
    "        df_sysout['time'] = pd.to_numeric(df_sysout.index, downcast='float')/1e9\n",
    "        df_sysout.set_index('time', inplace=True)\n",
    "\n",
    "        # This is probably not needed anymore, since it is all reshaped at the beginning\n",
    "        df_shifted = df_ist_soll.loc[df_ist_soll.x_soll != 0.0]\n",
    "        df_shifted.index = df_shifted.index - df_shifted.index[0]\n",
    "\n",
    "        df_concat = pd.concat([df_sysout,\n",
    "                               df_shifted],\n",
    "                              axis=1, join='inner', ignore_index=False)\n",
    "\n",
    "        mse = mean_squared_error(df_concat.normalized.values, df_concat[0].values)\n",
    "        #assert (mse == np.mean(np.square(df_concat[0]-df_concat.normalized))) # Just to understand what is going on\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print \"Exception\"\n",
    "        print e\n",
    "        #plot(y, t)\n",
    "        #df_sysout.plot()\n",
    "        mse = 1.0\n",
    "        \n",
    "    ## Test Tonys impl\n",
    "    try:\n",
    "        mse_tony = 1.0\n",
    "        #mse_tony = ident_tools.mse(df_ist_soll.normalized.values,\n",
    "        #                           df_ist_soll.index.values,\n",
    "        #                           y,\n",
    "        #                           t)\n",
    "    except ValueError as e:\n",
    "        print \"Exception\"\n",
    "        print e\n",
    "        mse_tony = 1.0\n",
    "\n",
    "    return mse, mse_tony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m1, m2 = get_mse(y1, t1, df_ist_soll[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, df_pos in enumerate(df_ist_soll):\n",
    "    for j, df_exp in enumerate(df_pos):\n",
    "        for key in model_estimations[i][j].keys():\n",
    "            if key in ['step_height', 'scale_factor']:\n",
    "                #skip\n",
    "                continue\n",
    "            print \"Processing MSE for Experiement[{0}][{1}][{2}]\".format(i,j, key)\n",
    "            t, y = con.step_response(model_estimations[i][j][key]['tf_delay'])\n",
    "            model_estimations[i][j][key]['y_step'] = pd.DataFrame({'time': t, 'y_step': y}).set_index('time')            \n",
    "\n",
    "            mse, mse_tony = get_mse(y, t, df_exp)\n",
    "            model_estimations[i][j][key]['mse'] = mse            \n",
    "            model_estimations[i][j][key]['mse_tony'] = mse_tony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_from_all_experiements_in_position(pos, est_sys_key, paramerter_key):\n",
    "    res = []\n",
    "    for j, exp in enumerate(model_estimations[pos]):\n",
    "        if est_sys_key in model_estimations[pos][j]:\n",
    "            res.append(model_estimations[pos][j][est_sys_key][paramerter_key])\n",
    "    return res\n",
    "\n",
    "def from_every_experiement(est_sys_key, paramerter_key):\n",
    "    res = []\n",
    "    for i, pos in enumerate(model_estimations):\n",
    "        for j, exp in enumerate(pos):\n",
    "            if est_sys_key in pos[j]:\n",
    "                res.append(exp[est_sys_key][paramerter_key])\n",
    "    return res\n",
    "\n",
    "def experiement_by_value(parameter_key):\n",
    "    result = []\n",
    "    position = []\n",
    "    for i, pos in enumerate(model_estimations):\n",
    "        for j, exp in enumerate(pos):\n",
    "            #print exp.keys()\n",
    "            for k, variant in enumerate(exp):\n",
    "                #print exp[variant]\n",
    "                if type(exp[variant]) is dict and parameter_key in exp[variant].keys():\n",
    "                    result.append(exp[variant][parameter_key])\n",
    "                    position.append( (i, j, variant, parameter_key) )\n",
    "    return result, position\n",
    "\n",
    "#print results_from_all_experiements_in_position(0, 'pt1_smooth_2', 'steady_state')\n",
    "#print\n",
    "#print from_every_experiement('pt1_smooth_2', 'steady_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print from_every_experiement('pt1_smooth_1', 'mse')\n",
    "#print from_every_experiement('pt1_smooth_2', 'mse')\n",
    "#print from_every_experiement('pt1_smooth_3', 'mse')\n",
    "#print experiement_by_value('mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing MSE against all data and average!\n",
    "# Get all step_responses\n",
    "res, pos = experiement_by_value('y_step') \n",
    "\n",
    "for df_step, (ii, jj, variant, _) in zip(res, pos):\n",
    "    print \"Processing\", ii, jj, variant\n",
    "    mse = model_estimations[ii][jj][variant]['mse']\n",
    "    \n",
    "    for i, df_pos in enumerate(df_ist_soll):\n",
    "        #print i\n",
    "        for j, df_exp in enumerate(df_pos):\n",
    "            #print j\n",
    "            if ii != i and jj != j:\n",
    "                #print df_step.y_step.values\n",
    "                _mse, _mse_tony = get_mse(df_step.y_step.values, df_step.index.values, df_exp)\n",
    "                mse = mse + _mse\n",
    "\n",
    "    mse = mse /(observations*positions)\n",
    "    model_estimations[ii][jj][variant]['mse_avg'] = mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res, pos = experiement_by_value('mse_avg')\n",
    "i, j, var, _ = pos[res.index(min(res))]\n",
    "print \"All time Best\", min(res), pos[res.index(min(res))]\n",
    "\n",
    "ax = model_estimations[i][j][var]['y_step'].plot()\n",
    "df_ist_soll[8][6]['normalized'].plot(ax=ax)\n",
    "\n",
    "print model_estimations[i][j][var]['tf_delay']\n",
    "print \"Poles:\\n\", model_estimations[i][j][var]['tf_delay'].pole()\n",
    "print \"Zeros:\\n\", model_estimations[i][j][var]['tf_delay'].zero()\n",
    "print\n",
    "print \"SS (without delay):\\n\", con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "print \"delay [s]:\", model_estimations[i][j][var]['delay']\n",
    "print \"SS (with delay):\\n\", con.tf2ss(model_estimations[i][j][var]['tf_delay'])\n",
    "\n",
    "plt.axvline(x=model_estimations[i][j][var]['delay'], color='black')\n",
    "\n",
    "\n",
    "poles = con.pole(model_estimations[i][j][var]['tf_delay'])\n",
    "\n",
    "fname = 'ss5_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'pade1_{}'.format(model_estimations[i][j][var]['delay']).replace('.', ':'))\n",
    "\n",
    "_ss = con.tf2ss(model_estimations[i][j][var]['tf_delay'])\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "poles = con.pole(model_estimations[i][j][var]['tf'])\n",
    "\n",
    "fname = 'ss5_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'nodelay')\n",
    "\n",
    "_ss = con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "#i, j, var, _ = pos[res.index(max(res))]\n",
    "#print \"Worst\", max(res), pos[res.index(max(res))]\n",
    "#model_estimations[i][j][var]['y_step'].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res, pos = experiement_by_value('mse_avg')\n",
    "i, j, var, _ = pos[res.index(min(res))]\n",
    "print \"All time Best\", min(res), pos[res.index(min(res))]\n",
    "\n",
    "#ax = model_estimations[i][j][var]['y_step'].plot()\n",
    "\n",
    "###########################################\n",
    "pade_cnt = 2\n",
    "\n",
    "###########################################\n",
    "\n",
    "tf = model_estimations[i][j][var]['tf']\n",
    "\n",
    "numerator, denominator = con.pade(delay, pade_cnt)\n",
    "delay2_tf           = con.tf(numerator,denominator)\n",
    "\n",
    "tf_delay2 = tf * delay2_tf\n",
    "\n",
    "df_ist_soll[8][6]['normalized'].plot()\n",
    "plt.plot(*con.step_response(tf_delay2))\n",
    "\n",
    "print \"Poles:\\n\", tf_delay2.pole()\n",
    "print \"Zeros:\\n\", tf_delay2.zero()\n",
    "print\n",
    "print \"SS (without delay):\\n\", con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "print \"delay [s]:\", model_estimations[i][j][var]['delay']\n",
    "print \"SS (with delay):\\n\", con.tf2ss(tf_delay2)\n",
    "\n",
    "plt.axvline(x=model_estimations[i][j][var]['delay'], color='black')\n",
    "\n",
    "\n",
    "poles = con.pole(tf_delay2)\n",
    "\n",
    "fname = 'ss6_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'pade{}_{}'.format(pade_cnt, model_estimations[i][j][var]['delay']).replace('.', ':'))\n",
    "\n",
    "_ss = con.tf2ss(tf_delay2)\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "poles = con.pole(model_estimations[i][j][var]['tf'])\n",
    "\n",
    "fname = 'ss6_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'nodelay')\n",
    "\n",
    "_ss = con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "#i, j, var, _ = pos[res.index(max(res))]\n",
    "#print \"Worst\", max(res), pos[res.index(max(res))]\n",
    "#model_estimations[i][j][var]['y_step'].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, pos = experiement_by_value('mse_avg')\n",
    "# Clean up unwanted stuff\n",
    "\n",
    "wanted_key = 'pt1'\n",
    "\n",
    "idx = []\n",
    "for p, r in zip(pos, res):\n",
    "    for key in p:\n",
    "        if wanted_key in str(key):\n",
    "            idx.append(pos.index(p))\n",
    "\n",
    "clean_res = [res[i] for i in idx]\n",
    "clean_pos = [pos[i] for i in idx]\n",
    "\n",
    "pos = clean_pos\n",
    "res = clean_res\n",
    "\n",
    "###########################################################\n",
    "\n",
    "i, j, var, _ = pos[res.index(min(res))]\n",
    "print \"Best {}:\".format(wanted_key), min(res), pos[res.index(min(res))]\n",
    "\n",
    "###########################################\n",
    "pade_cnt = 1\n",
    "\n",
    "###########################################\n",
    "\n",
    "tf = model_estimations[i][j][var]['tf']\n",
    "\n",
    "numerator, denominator = con.pade(delay, pade_cnt)\n",
    "delay2_tf           = con.tf(numerator,denominator)\n",
    "\n",
    "tf_delay2 = tf * delay2_tf\n",
    "\n",
    "df_ist_soll[8][6]['normalized'].plot()\n",
    "plt.plot(*con.step_response(tf_delay2))\n",
    "\n",
    "print \"Poles:\\n\", tf_delay2.pole()\n",
    "print \"Zeros:\\n\", tf_delay2.zero()\n",
    "print\n",
    "print \"SS (without delay):\\n\", con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "print \"delay [s]:\", model_estimations[i][j][var]['delay']\n",
    "print \"SS (with delay):\\n\", con.tf2ss(tf_delay2)\n",
    "\n",
    "plt.axvline(x=model_estimations[i][j][var]['delay'], color='black')\n",
    "\n",
    "\n",
    "poles = con.pole(tf_delay2)\n",
    "\n",
    "fname = 'ss1_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'pade{}_{}'.format(pade_cnt, model_estimations[i][j][var]['delay']).replace('.', ':'))\n",
    "\n",
    "_ss = con.tf2ss(tf_delay2)\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "poles = con.pole(model_estimations[i][j][var]['tf'])\n",
    "\n",
    "fname = 'ss1_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'nodelay')\n",
    "\n",
    "_ss = con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "#i, j, var, _ = pos[res.index(max(res))]\n",
    "#print \"Worst\", max(res), pos[res.index(max(res))]\n",
    "#model_estimations[i][j][var]['y_step'].plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, pos = experiement_by_value('mse_avg')\n",
    "# Clean up unwanted stuff\n",
    "\n",
    "wanted_key = 'pt1'\n",
    "\n",
    "idx = []\n",
    "for p, r in zip(pos, res):\n",
    "    for key in p:\n",
    "        if wanted_key in str(key):\n",
    "            idx.append(pos.index(p))\n",
    "\n",
    "clean_res = [res[i] for i in idx]\n",
    "clean_pos = [pos[i] for i in idx]\n",
    "\n",
    "pos = clean_pos\n",
    "res = clean_res\n",
    "\n",
    "###########################################################\n",
    "\n",
    "i, j, var, _ = pos[res.index(min(res))]\n",
    "print \"Best {}:\".format(wanted_key), min(res), pos[res.index(min(res))]\n",
    "\n",
    "###########################################\n",
    "pade_cnt = 2\n",
    "\n",
    "###########################################\n",
    "\n",
    "tf = model_estimations[i][j][var]['tf']\n",
    "\n",
    "numerator, denominator = con.pade(delay, pade_cnt)\n",
    "delay2_tf           = con.tf(numerator,denominator)\n",
    "\n",
    "tf_delay2 = tf * delay2_tf\n",
    "\n",
    "df_ist_soll[8][6]['normalized'].plot()\n",
    "plt.plot(*con.step_response(tf_delay2))\n",
    "\n",
    "print \"Poles:\\n\", tf_delay2.pole()\n",
    "print \"Zeros:\\n\", tf_delay2.zero()\n",
    "print\n",
    "print \"SS (without delay):\\n\", con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "print \"delay [s]:\", model_estimations[i][j][var]['delay']\n",
    "print \"SS (with delay):\\n\", con.tf2ss(tf_delay2)\n",
    "\n",
    "plt.axvline(x=model_estimations[i][j][var]['delay'], color='black')\n",
    "\n",
    "\n",
    "poles = con.pole(tf_delay2)\n",
    "\n",
    "fname = 'ss2_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'pade{}_{}'.format(pade_cnt, model_estimations[i][j][var]['delay']).replace('.', ':'))\n",
    "\n",
    "_ss = con.tf2ss(tf_delay2)\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "poles = con.pole(model_estimations[i][j][var]['tf'])\n",
    "\n",
    "fname = 'ss2_{}_poles{}_ident_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                    len(poles), 'nodelay')\n",
    "\n",
    "_ss = con.tf2ss(model_estimations[i][j][var]['tf'])\n",
    "loadtools.saveDelayModel(fname, _ss.A, _ss.B, _ss.C, _ss.D, model_estimations[i][j][var]['delay'])\n",
    "#loadtools.saveModel(fname, _ss.A, _ss.B, _ss.C, _ss.D)\n",
    "#loadtools.saveNPY(fname, delay=model_estimations[i][j][var]['delay'])\n",
    "\n",
    "\n",
    "#i, j, var, _ = pos[res.index(max(res))]\n",
    "#print \"Worst\", max(res), pos[res.index(max(res))]\n",
    "#model_estimations[i][j][var]['y_step'].plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for i, pos in enumerate(model_estimations):\n",
    "    for j, exp in enumerate(pos):\n",
    "        if 'pt2' in pos[j].keys():\n",
    "            print i, j\n",
    "            print con.pole(pos[j]['pt2']['tf_delay'])\n",
    "            print\n",
    "        if 'pt2_smooth_1' in pos[j].keys():\n",
    "            print i, j\n",
    "            print con.pole(pos[j]['pt2_smooth_1']['tf_delay'])\n",
    "            print\n",
    "        if 'pt2_smooth_2' in pos[j].keys():\n",
    "            print i, j\n",
    "            print con.pole(pos[j]['pt2_smooth_2']['tf_delay'])\n",
    "            print\n",
    "        if 'pt2_smooth_3' in pos[j].keys():\n",
    "            print i, j\n",
    "            print con.pole(pos[j]['pt2_smooth_3']['tf_delay'])\n",
    "            print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### State Space Parameters of all the ideal models in a textfile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(\"State_Space_Parameters_pt1.txt\", \"w\") as text_file:\n",
    "    text_file.write('###########################\\n')\n",
    "    text_file.write('  STATE SPACE PARAMETERS   \\n')\n",
    "    text_file.write('###########################\\n')\n",
    "    for i in range(0, positions):\n",
    "        text_file.write('\\nPosition %s\\n'%(i+1))\n",
    "        text_file.write('%s'%system_matrix_pt1[i])\n",
    "        text_file.write('\\n')\n",
    "        \n",
    "with open(\"State_Space_Parameters_pt2.txt\", \"w\") as text_file:\n",
    "    text_file.write('###########################\\n')\n",
    "    text_file.write('  STATE SPACE PARAMETERS   \\n')\n",
    "    text_file.write('###########################\\n')\n",
    "    for i in range(0, positions):\n",
    "        text_file.write('\\nPosition %s\\n'%(i+1))\n",
    "        text_file.write('%s'%system_matrix_pt2[i])\n",
    "        text_file.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "try:\n",
    "    os.makedirs('Model_Validation/')\n",
    "except OSError, e:\n",
    "    if e.errno != os.errno.EEXIST:\n",
    "        raise   \n",
    "    pass\n",
    "for i in range(0, positions):\n",
    "    fig = plt.figure(figsize = (5,4))\n",
    "    fig.suptitle('Position %s'%(i + 1), fontsize = 20, fontweight = 'bold')\n",
    "    plt.plot(model_time_pt2[i], model_output_pt2[i], '--r', label = 'ideal pt2 model')\n",
    "    plt.plot(model_time_pt1[i], model_output_pt1[i], '--b', label = 'ideal pt1 model')\n",
    "    plt.legend()\n",
    "    for j in range(0, observations):\n",
    "        try:\n",
    "            \n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.plot(df_ist_soll[i][j] * m_factor_array[j]) \n",
    "        except:\n",
    "            pass\n",
    "    plt.savefig('Model_Validation/Position %s model.png'%(i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
